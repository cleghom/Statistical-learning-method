{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章 统计学习方法概论\n",
    "## 统计学习\n",
    "  简述之，从数据中挖掘规律，并形式化表现这种规律并将之应用在将来的数据之中，其实现在的机器学习方法都是离不开统计学原理的，这也就是本人结合李航的《统计学习方法》来编写统计学习的相关基础算法的初衷。一些基本的概念这里不再赘述，统计学习方法包含了监督式学习、非监督式学习、半监督式学习和强化学习等。这里也先假定每个读者对这些概念已经有了一些初步的了解，如果不了解，也没有关系，这个教程将会在之后的章节里对这些概念进行实际地操作，尽可能会让每个读者都能有所收获。  \n",
    "  另外，由于原书实际是一本教材，且某些概念之所以出现也是属于按照教材编排的风格设计的，但这样一开始就涉及了大量的基本概念，对于想从事机器学习的读者来说，可能会吃不消，这里我将只会在第一章编写最需要要理解的概念，至于其他的概念，本教程将在具体应用到时候具体描述和讲解。\n",
    "  \n",
    "**本章删繁就简，只提炼原书的核心内容：**\n",
    "* 输入空间、特征空间、输出空间：  \n",
    ">每个具体的样本实例（$instance$）在统计学习方法中实际表示为一个向量，叫做**特征向量**，而这个特征向量所存在的空间，称之为**特征空间**，通常这个向量表示为列向量$x$，即有\n",
    "$$x = \\left( x^\\left(1\\right),x^\\left(2\\right),...,x^\\left(i\\right),...,x^\\left(n\\right) \\right)^T \n",
    "= \\begin{bmatrix} x^\\left(1\\right) \\\\ x^\\left(2\\right) \\\\ ... \\\\ x^\\left(i\\right) \\\\ ... \\\\ x^\\left(n\\right) \\end{bmatrix}$$ \n",
    "其中，$x^\\left(i\\right)$表示$x$的第$i$个特征，注意，$x^\\left(i\\right)$和$x_i$不同，这里通常用$x_i$表示多个输入变量的第$i$个，即有\n",
    "$$x_i = \\left( x_i^\\left(1\\right),x_i^\\left(2\\right),...,x_i^\\left(i\\right),...,x_i^\\left(n\\right) \\right)^T\n",
    "= \\begin{bmatrix} x_i^\\left(1\\right) \\\\ x_i^\\left(2\\right) \\\\ ... \\\\ x_i^\\left(i\\right) \\\\ ... \\\\ x_i^\\left(n\\right) \\end{bmatrix}$$ \n",
    "当有许多$x$组成所有的输入时，记为$X$，$X$中的所有的$x_i$所对应的$y_i$记为$Y$，$Y$是一个类别标签向量，通常对于“类别标签”这个词，简称为“类标”。\n",
    "* 补充\n",
    ">读者可能会好奇，通常一行$x$对应一个$y$是很好理解的，如下所示，其中，$X$数据包含了$m$个实例，每个实例有$n$个属性：\n",
    "$$\n",
    "\\begin{bmatrix} X & Y \\\\ \\end{bmatrix} =\\begin{bmatrix} \n",
    "x_1 & y_1 \\\\\n",
    "x_2 & y_2 \\\\\n",
    "... & ... \\\\\n",
    "x_i & y_i \\\\ \n",
    "... & ... \\\\\n",
    "x_m & y_m \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1^\\left( 1 \\right) & x_1^\\left( 2 \\right) & x_1^\\left( 3 \\right) & ... & x_1^\\left( n \\right) & y_1 \\\\\n",
    "x_2^\\left( 1 \\right) & x_2^\\left( 2 \\right) & x_2^\\left( 3 \\right) & ... & x_2^\\left( n \\right) & y_2 \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "x_i^\\left( 1 \\right) & x_i^\\left( 2 \\right) & x_i^\\left( 3 \\right) & ... & x_i^\\left( n \\right) & y_i \\\\ \n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "x_m^\\left( 1 \\right) & x_m^\\left( 2 \\right) & x_m^\\left( 3 \\right) & ... & x_m^\\left( n \\right) & y_m \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "为什么要把$x$弄成列呢？原因是这这样的，由于在之后的统计学习的方法中，需要反复使用一个式子，即$f\\left(x\\right) = w \\cdot x+b$，即“**参数**乘以**样本**加**偏置项**”，此时，从矩阵运算角度来讲，$w$是横向量，$x$是列向量，$w \\cdot x+b$的结果$f\\left(x\\right)$就成为了一个常数项，如果不这样，$x$表现为上图中的横向量，那就需要将这个$w$和$x$就需要来回倒腾，很麻烦，所以，对于数学分析来讲，这里定义$x$是一个列向量是极好的。但是直观来讲，上面的式子对整个训练集的表现形式更符合人类直观的理解(至少我很喜欢这种表现形式，主要看着好看哈哈哈，= =～)。\n",
    "* 关于统计学习方法以及机器学习方法的本质描述  \n",
    ">由$X$决定$Y$的过程实际是一个**模型**所要所的事情，这个模型，可以有两种理解：第一，是一个概率关系，即有$X$的分布来确定$Y$的分布，并且它是一种条件概率分布的关系，有$$P\\left( Y |X\\right)$$\n",
    "第二种，即函数关系，当有$X$的时候，可以通过一种函数关系映射将之表现为$Y$，此时，有\n",
    "$$Y=f\\left(X\\right)$$\n",
    "当新的样本进来要用模型去预测其可能的结果的时候，可以形式化得将刚才得到的$P$和$f\\left(\\right)$应用在新的样本$x$上，则预测的过程记为$P\\left(y|x\\right)$或者$y=f\\left(x\\right)$\n",
    "\n",
    "至于原书中其他的概念，如**损失函数**、**过拟合与欠拟合**等，本文不打算详细描述了，这个一方面需要弄懂的话也不是简简单单描述完就没事儿了，还是需要动手，之后每个章节对算法进行描述的时候，若是涉及到了相关概念，也会具体说明一下，而且结合案例来讲概念，或许更有利于理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
